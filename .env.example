# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=

# Collection Names
QA_COLLECTION_NAME=qa_pairs
TEXT_COLLECTION_NAME=plain_text

# Model Paths (can be HuggingFace model IDs or local paths)
LLM_MODEL_PATH=Qwen/Qwen3-Next-80B-A3B-Instruct
EMBEDDING_MODEL_PATH=Qwen/Qwen3-Embedding-4B
RERANKER_MODEL_PATH=Qwen/Qwen3-Reranker-4B

# Model Configuration
LLM_MAX_NEW_TOKENS=16384
LLM_TEMPERATURE=0.7
EMBEDDING_MAX_LENGTH=8192
RERANKER_MAX_LENGTH=8192

# Retrieval Configuration
QUERY_VARIANTS_COUNT=3
TOP_K_PER_QUERY=15
CANDIDATES_BEFORE_RERANK=30
FINAL_TOP_K=7
RERANKER_THRESHOLD=0.5
MMR_DIVERSITY_SCORE=0.3

# Hybrid Search Weights
QA_DENSE_WEIGHT=0.3
QA_SPARSE_WEIGHT=0.7
TEXT_DENSE_WEIGHT=0.7
TEXT_SPARSE_WEIGHT=0.3

# RRF Configuration
RRF_K=60

# Device Configuration
DEVICE=cuda  # cuda, cpu, or auto

# vLLM Configuration
VLLM_SERVER_URL=http://localhost:8000
VLLM_MODEL_NAME=qwen3-next
VLLM_TENSOR_PARALLEL_SIZE=4
VLLM_MAX_MODEL_LEN=262144  # 256K context window

# Generation Settings
GENERATION_TEMPERATURE=0.7
GENERATION_TOP_P=0.9
GENERATION_MAX_TOKENS=1024
GENERATION_MIN_TOKENS=50
ENABLE_STREAMING=true

# Conversation History Settings
MAX_CONVERSATION_TOKENS=200000  # Safe limit (256K available)
CONVERSATION_WARNING_THRESHOLD=0.8  # Warn at 80% (160K tokens)
AUTO_SUMMARIZE_OLD_MESSAGES=false  # Enable conversation summarization
MESSAGES_TO_KEEP_FULL=20  # Keep last N messages at full detail

# Context Settings for Generation
MAX_CONTEXTS_FOR_GENERATION=7
MAX_CONTEXT_TOKENS=3000
INCLUDE_CITATIONS=true

# Response Settings
MIN_CONFIDENCE_THRESHOLD=0.6
FALLBACK_RESPONSE=Üzgünüm, bu konuda yeterli bilgim yok.
