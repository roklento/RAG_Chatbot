# Colab Environment Configuration
# This config is optimized for Google Colab with Gemma-3n-E4B-it

# ============================================================================
# MODEL PATHS (Will be set in Colab)
# ============================================================================
EMBEDDING_MODEL_PATH=/content/models/Qwen3-Embedding-4B
RERANKER_MODEL_PATH=/content/models/Qwen3-Reranker-4B
GENERATION_MODEL_PATH=/content/models/gemma-3n-E4B-it

# ============================================================================
# DEVICE SETTINGS
# ============================================================================
DEVICE=cuda
EMBEDDING_DEVICE=cuda
RERANKER_DEVICE=cuda
GENERATION_DEVICE=cuda

# ============================================================================
# QDRANT SETTINGS (In-Memory for Colab)
# ============================================================================
QDRANT_HOST=:memory:
QDRANT_PORT=6333
QDRANT_COLLECTION_QA=qa_pairs
QDRANT_COLLECTION_TEXT=plain_text

# ============================================================================
# EMBEDDING SETTINGS (UNCHANGED)
# ============================================================================
EMBEDDING_DIMENSION=1024
EMBEDDING_BATCH_SIZE=32

# ============================================================================
# RETRIEVAL SETTINGS (UNCHANGED)
# ============================================================================
# Query Processing
ENABLE_QUERY_CORRECTION=true
ENABLE_QUERY_DIVERSIFICATION=true
DIVERSIFICATION_COUNT=2

# Hybrid Search Weights
QA_PAIRS_DENSE_WEIGHT=0.3
QA_PAIRS_SPARSE_WEIGHT=0.7
PLAIN_TEXT_DENSE_WEIGHT=0.7
PLAIN_TEXT_SPARSE_WEIGHT=0.3

# Reranking
ENABLE_RERANKING=true
RERANKER_TOP_K=20

# MMR Diversity
MMR_DIVERSITY_WEIGHT=0.3
MMR_RELEVANCE_WEIGHT=0.7

# ============================================================================
# GENERATION SETTINGS (OPTIMIZED FOR GEMMA-3N)
# ============================================================================
# Model settings
GENERATION_MODEL_NAME=google/gemma-3n-E4B-it
USE_VLLM=false
USE_TRANSFORMERS=true
GENERATION_DTYPE=bfloat16

# Context and token limits (ADJUSTED FOR COLAB)
MAX_MODEL_LEN=32768  # Gemma-3n supports 32K
MAX_CONVERSATION_TOKENS=8000  # Conservative limit for demo
CONVERSATION_WARNING_THRESHOLD=0.8
MAX_CONTEXTS_FOR_GENERATION=5  # Reduced from 7 for memory
MAX_RECENT_MESSAGES=10  # Reduced from 20

# Generation parameters
GENERATION_TEMPERATURE=0.7
GENERATION_TOP_P=0.9
GENERATION_TOP_K=50
GENERATION_MAX_TOKENS=512  # Response length limit
GENERATION_MIN_TOKENS=50
ENABLE_STREAMING=true

# ============================================================================
# COLAB-SPECIFIC SETTINGS
# ============================================================================
# Memory optimization
LOAD_IN_8BIT=false  # Set true if running low on VRAM
LOAD_IN_4BIT=false  # Set true for very limited VRAM
USE_FLASH_ATTENTION=false  # Set true if available

# Batch processing
GENERATION_BATCH_SIZE=1  # Process one query at a time in Colab

# Logging
LOG_LEVEL=INFO
VERBOSE=true
