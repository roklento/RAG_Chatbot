# Advanced RAG Chatbot

An advanced Retrieval-Augmented Generation (RAG) chatbot system with state-of-the-art retrieval techniques.

## ğŸŒŸ Features

### Advanced Retrieval Pipeline

1. **Query Processing**
   - Automatic spelling and grammar correction
   - Query diversification (3-5 semantic variants)
   - Context-aware query understanding
   - Powered by Qwen3-Next-80B-A3B-Instruct

2. **Hybrid Search**
   - Dense (semantic) + Sparse (keyword) retrieval
   - Dual collection architecture (Q&A pairs vs Plain text)
   - Collection-specific weight optimization
   - Multi-query fusion with Reciprocal Rank Fusion (RRF)

3. **Reranking**
   - Cross-encoder reranking with Qwen3-Reranker-4B
   - Relevance threshold filtering
   - Instruction-based tuning support

4. **Diversity Optimization**
   - Maximal Marginal Relevance (MMR) filtering
   - Balances relevance with diversity
   - Configurable diversity weights

## ğŸ—ï¸ Architecture

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Processing (Qwen3-Next-80B)   â”‚
â”‚  - Correction                        â”‚
â”‚  - 3-5 variants generation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hybrid Multi-Query Search           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Q&A Pairs    â”‚  â”‚ Plain Text   â”‚â”‚
â”‚  â”‚ Dense: 30%   â”‚  â”‚ Dense: 70%   â”‚â”‚
â”‚  â”‚ Sparse: 70%  â”‚  â”‚ Sparse: 30%  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚         RRF Fusion                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reranking (Qwen3-Reranker-4B)       â”‚
â”‚  - Cross-encoder scoring             â”‚
â”‚  - Threshold filtering (>0.5)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MMR Diversity Filtering             â”‚
â”‚  - Balance relevance & diversity     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Results (Top 5-10)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (recommended for 80B model)
- Qdrant vector database

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd RAG_Chatbot
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Setup Qdrant:
```bash
# Using Docker
docker run -p 6333:6333 qdrant/qdrant

# Or install locally
# See: https://qdrant.tech/documentation/guides/installation/
```

4. Configure environment:
```bash
cp .env.example .env
# Edit .env with your settings
```

### Usage

#### 1. Setup Database

```bash
cd examples
python 01_setup_database.py
```

This will:
- Create Qdrant collections
- Add sample Q&A pairs
- Add sample plain text documents

#### 2. Test Retrieval

```bash
python 02_test_retrieval.py
```

This demonstrates the full retrieval pipeline with verbose output.

#### 3. Use in Your Code

```python
from src.config import get_settings
from src.retrieval.advanced_retriever import create_advanced_retriever

# Initialize
settings = get_settings()
retriever = create_advanced_retriever(settings)

# Simple retrieval
results = retriever.retrieve_simple(
    query="What is machine learning?",
    top_k=5
)

for doc in results:
    print(doc)

# Detailed retrieval
results = retriever.retrieve(
    query="Explain transformers",
    top_k=5,
    apply_mmr=True,
    verbose=True
)

for result in results:
    print(f"Score: {result.relevance_score:.4f}")
    print(f"Content: {result.content}")
    print(f"Collection: {result.collection}")
```

## ğŸ“ Project Structure

```
RAG_Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/           # Configuration management
â”‚   â”œâ”€â”€ models/           # Model wrappers
â”‚   â”‚   â”œâ”€â”€ embedding.py      # Qwen3-Embedding-4B
â”‚   â”‚   â”œâ”€â”€ reranker.py       # Qwen3-Reranker-4B
â”‚   â”‚   â””â”€â”€ query_processor.py # Qwen3-Next-80B
â”‚   â”œâ”€â”€ retrieval/        # Retrieval components
â”‚   â”‚   â”œâ”€â”€ qdrant_manager.py      # Database management
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py    # Hybrid search
â”‚   â”‚   â””â”€â”€ advanced_retriever.py  # Main orchestrator
â”‚   â””â”€â”€ utils/            # Utility functions
â”‚       â”œâ”€â”€ fusion.py     # RRF implementation
â”‚       â””â”€â”€ mmr.py        # MMR implementation
â”œâ”€â”€ examples/             # Example scripts
â”œâ”€â”€ tests/               # Test files
â”œâ”€â”€ data/                # Data directory
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ .env.example        # Environment template
â””â”€â”€ README.md           # This file
```

## âš™ï¸ Configuration

Key configuration parameters in `.env`:

```bash
# Models
LLM_MODEL_PATH=Qwen/Qwen3-Next-80B-A3B-Instruct
EMBEDDING_MODEL_PATH=Qwen/Qwen3-Embedding-4B
RERANKER_MODEL_PATH=Qwen/Qwen3-Reranker-4B

# Retrieval Settings
QUERY_VARIANTS_COUNT=3          # Number of query variants
TOP_K_PER_QUERY=15              # Results per query per collection
CANDIDATES_BEFORE_RERANK=30     # Candidates before reranking
FINAL_TOP_K=7                   # Final results after all filtering
RERANKER_THRESHOLD=0.5          # Min reranker score
MMR_DIVERSITY_SCORE=0.3         # Diversity weight (0-1)

# Hybrid Search Weights
QA_DENSE_WEIGHT=0.3             # Q&A: Dense search weight
QA_SPARSE_WEIGHT=0.7            # Q&A: Sparse search weight
TEXT_DENSE_WEIGHT=0.7           # Text: Dense search weight
TEXT_SPARSE_WEIGHT=0.3          # Text: Sparse search weight

# RRF
RRF_K=60                        # RRF constant (standard: 60)
```

## ğŸ”¬ Models Used

| Component | Model | Size | Purpose |
|-----------|-------|------|---------|
| Query Processing | Qwen3-Next-80B-A3B-Instruct | 80B | Correction & diversification |
| Embedding | Qwen3-Embedding-4B | 4B | Semantic embeddings |
| Reranking | Qwen3-Reranker-4B | 4B | Relevance scoring |

## ğŸ“Š Performance Considerations

### Latency Breakdown (Estimated)

- **Query Processing**: 2-4s (80B LLM)
- **Hybrid Search**: 0.2-0.5s (Qdrant)
- **Reranking**: 0.3-0.6s (4B model, 30 candidates)
- **MMR**: <0.1s (numpy operations)
- **Total**: ~3-5 seconds per query

### Optimization Tips

1. **For Production**: Consider using smaller model (Qwen2.5-14B) for query processing
2. **GPU Memory**: 80B model requires ~80GB VRAM (quantized) or ~160GB (FP16)
3. **Batch Processing**: Process multiple queries in parallel when possible
4. **Caching**: Cache query embeddings for repeated queries

## ğŸ§ª Testing

Run component tests:

```bash
cd examples
python 03_component_testing.py
```

This allows testing individual components:
- Query processor
- Embedding model
- Reranker
- Hybrid retriever

## ğŸ“š Advanced Usage

### Custom Data Ingestion

```python
from src.config import get_settings
from src.models import QwenEmbedding
from src.retrieval import QdrantManager

settings = get_settings()
embedding_model = QwenEmbedding(settings.embedding_model_path)
manager = QdrantManager(settings, embedding_model)

# Add Q&A pairs
qa_pairs = [
    {"question": "...", "answer": "..."},
    # ...
]
manager.add_qa_pairs(qa_pairs)

# Add plain text
texts = ["Document 1...", "Document 2...", ...]
manager.add_plain_text(texts)
```

### Custom Retrieval Pipeline

```python
from src.retrieval import AdvancedRetriever

# Custom settings
results = retriever.retrieve(
    query="Your query",
    top_k=10,              # More results
    apply_mmr=False,       # Disable diversity
    verbose=True,          # Show pipeline steps
)
```

## ğŸ¤ Contributing

This is a demonstration project showcasing advanced RAG techniques. Feel free to extend and customize for your use case.

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- Qwen Team for excellent open-source models
- Qdrant for the vector database
- LangChain and LlamaIndex communities

## ğŸ“ Support

For issues and questions, please open an issue on the repository.

---

**Note**: This is the retrieval component of the RAG system. The generation component (using the same Qwen3-Next-80B model) will be implemented in the next phase.
